# 论文笔记--Tool Learning

## 1. ToolRL: Reward is All Tool Learning Needs


- **拟解决的问题**：提高大模型的工具调用能力。

- **采用的方法**：**强化学习**进行后训练的方法，**GRPO**（去掉KL约束项，以更快的收敛）；奖励为**格式奖励**和**正确性奖励**的和，$R_{\text{final}} = R_{\text{final}} + R_{\text{final}} \in [-3, 4]$。

- **主要观察**：
    
    1. **SFT**相比较**RL**的缺点是更容易**过拟合**
    
    2. 更长的推理（“**多思考**”）看起来是好事，但对于“**工具使用**”这类任务，并**不有益**

    3. 对于RL来说，**reward**是很关键的，论文重点探讨reward的不同设计模式

    4. 在对**格式奖励**和**正确性奖励**进行加权时，**不能突变**，突变让GRPO的训练结果变得更差。

## 2. ToolACE: Winning the Points of LLM Function Calling


- **拟解决的问题**：构造大模型的**函数调用数据集**

- **采用的方法**：1、合成API；2、模拟用户/助手/工具三种角色的真实交互，生成数据；3、过滤，采用规则检查（语法/参数）和模型检查（一致性/幻觉）

- **主要观察**：
    
    1. 对合成数据的有效使用可以提高LLM的能力
    
    2. 生成的数据必须与模型的能力相匹配，不能用同一套数据训练所有模型。

        
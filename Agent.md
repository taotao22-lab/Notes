# Agent后训练笔记

## 1. GRPO


- **核心目标**：对于一个给定的问题，在模型生成的多个答案中，相对更“好”的答案应该被鼓励，相对更“差”的答案应该被抑制。

- **主要步骤**：
    
    1. **创建“小组”**：

        对于一个用户的问题Q，我们不让模型只生成一个答案，而是让它生成多个（比如4个）不同的答案。这4个答案就构成了一个“小组”。

        - 成员1（答案1）：$s_1$，奖励分数$r_1$
        - 成员2（答案2）：$s_2$，奖励分数$r_2$
        - 成员3（答案3）：$s_3$，奖励分数$r_3$
        - 成员4（答案4）：$s_4$，奖励分数$r_4$
    
    2. **评估“小组整体表现”**：

        计算这个小组分数的**平均值($\mu_Q$)**和**标准差($\sigma_Q$)**：

        $$\mu_Q = \frac{1}{n} \sum_{i=1}^n r_i $$

        $$\sigma_Q = \sqrt{\frac{1}{n} \sum_{i=1}^n (r_i-\mu_Q)^2}$$

        - **$\mu_Q$的含义**：这个问题下，模型当前的平均表现水平
        - **$\sigma_Q$的含义**：这个小组的分数有多分散。如果所有答案得分都差不多，$\sigma_Q$就小；如果答案分数差异很大，$\sigma_Q$就大。

    3. **计算“相对优势”--组内比较的核心**：

        对于小组中的**每一个**答案$s_i$，我们计算一个**优势分数**$A_i$，这个分数反映了它在小组内的相对位置。公式如下：

        $$A_i(s_i|Q)=\frac{r_i-\mu_Q}{\sigma_Q+\eta}$$

        - $r_i-\mu_Q$ **(你的得分-小组平均分)**：
            
            - 如果是**正数**，说明你的表现**高于小组平均水平**
            - 如果是**负数**，说明你的表现**低于小组平均水平**
            - 这就是最直接的“组内比较”
        
        - **除以$\sigma_Q$ (小组分数的波动程度)**：

            - 它的作用是：**让比较更公平**
            - **如果小组分数很集中（$\sigma_Q$很小）**：那么即使你只比平均分高一点点，除以一个很小的数后，你的优势分数$A_i$也会被放大，说明你**很突出**。
            - **如果小组分数很分散（$\sigma_Q$很大）**：说明大家的得分本来就差异很大。这是你需要比平均分高出很多，才能获得一个较高的优势分数。
        
        - **$\eta$（一个很小的常数）**：防止分母为0

        $A_i$**的最终含义**：
        
        - $A_i>0$：你的答案比同一个问题下的其他答案**更好**
        - $A_i<0$：你的答案比同一个问题下的其他答案**更差**
        - $| A_i |$**（绝对值）**：你比平均水平好/差的**程度**

    4. **用“优势分数”来指导学习（更新模型）**：

        现在我们有了每个答案的“优势分数”$A_i$。最后一步就是利用这个分数来更新参数的模型，让它学到什么是好的，什么是不好的。

        这个过程发生在GRPO的目标函数中：

        - **如果$A_i$是正数（好答案）**：算法会**增加**模型生成类似这个答案的概率。
        - **如果$A_i$是负数（差答案）**：算法会**降低**模型生成类似这个答案的概率。

        因为所有比较都是在同一个问题的内部进行的，所以模型学到的不是“要获得10分”，而是在“处理这类问题时，这种思维模式和工具调用方式比其他我尝试的方式更有效”

        $$ J(\theta) = \mathbb{E}_{Q \sim D} \frac{1}{G} \sum_{i=1}^G \left[ \min (\frac{\pi_{\theta}(s_i|Q)}{\pi_{old}(s_i|Q)}A_i(s_i|Q), clip(\frac{\pi_{\theta}(s_i|Q)}{\pi_{old}(s_i|Q)}), 1-\epsilon, 1+\epsilon) A_i(s_i|Q) \right] - \beta \, D_{\text{KL}}\left( \pi_{\theta}\|\, \pi_{\text{old}} \right) $$

         $\beta D_{KL}$是KL散度约束，用于防止策略过度偏离参考策略

- **训练流程**：先推理，再训练

    完整训练**循环**：

        循环开始
            ↓
        推理阶段（生成数据）
            |-- 使用当前模型作为"旧策略"
            |-- 为每个prompt生成一组答案（如4个）
            |-- 计算每个答案的奖励
            |-- 计算组内相对优势
            ↓
        训练阶段（更新参数）
            |-- 固定使用推理阶段生成的数据
            |-- 多次epoch训练（通常3-8次）
            |-- 计算新旧策略概率比
            |-- 基于优势信号更新模型参数
            ↓
        模型参数更新完成 → 成为新的"旧策略"
            ↓
        回到循环开始

    时间线：
    
        t0: [模型 M0] --推理--> 生成数据，计算 π_M0(a|s) 并保存为 π_old
                            ↓
                            保存：{prompt, response, π_old, reward, advantage}
                            ↓
        t1: [模型 M0] --训练开始--> 用这批数据训练
                            ↓
                            第一次forward：计算 π_M0(a|s) （此时还是M0）
                            ↓
                            optimizer.step() → 模型变为 M1
                            ↓
        t2: [模型 M1] --继续训练--> 同一批数据，但模型变了
                            ↓
                            第二次forward：计算 π_M1(a|s) （已经是M1了！）
                            ↓
                            optimizer.step() → 模型变为 M2



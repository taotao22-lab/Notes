# 论文笔记--Tool Data Synthesis & Learning

## 1. <font color="red">ToolRL</font>: Reward is All Tool Learning Needs


- **拟解决的问题**：提高大模型的工具调用能力。

- **采用的方法**：**强化学习**进行后训练的方法，**GRPO**（去掉KL约束项，以更快的收敛）；奖励为**格式奖励**和**正确性奖励**的和，$R_{\text{final}} = R_{\text{format}} + R_{\text{correct}} \in [-3, 4]$。

- **主要观察**：
    
    1. **SFT**相比较**RL**的缺点是更容易**过拟合**（这篇论文的立足点）
    
    2. 更长的推理（“**多思考**”）看起来是好事，但对于“**工具使用**”这类任务，并**不有益**

    3. 对于RL来说，**reward**是很关键的，论文重点探讨reward的不同设计模式

    4. 在对**格式奖励**和**正确性奖励**进行加权时，**不能突变**，突变让GRPO的训练结果变得更差。

## 2. <font color="blue">（数据合成）</font> <font color="red">ToolACE</font>: Winning the Points of LLM Function Calling


- **拟解决的问题**：构造大模型的**函数调用数据集**

- **采用的方法**：1、合成API；2、模拟用户/助手/工具三种角色的真实交互，生成数据；3、过滤，采用规则检查（语法/参数）和模型检查（一致性/幻觉）

- **主要观察**：
    
    1. 对**合成数据的有效使用**可以提高LLM的能力
    
    2. 生成的数据必须与模型的能力相匹配，不能用同一套数据训练所有模型。


## 3. <font color="red">Nemotron-Research-Tool-N1</font>: Exploring Tool-Using Language Models with Reinforced Reasoning


- **拟解决的问题**：提升LLM使用外部工具时的推理能力

- **采用的方法**：强化学习的方法，使用GRPO算法，设计二元奖励函数（**结构和内容完全正确才奖励为1**，否则奖励为0）

- **主要观察**：
    
    1. 把**一个多轮数据**转换为**多个单轮数据**，ToolRL也是这么干的

    2. R1 风格的强化学习**无需精心准备推理轨迹**即可获得推理技能，这使得直接从现有的监督微调数据中训练推理能力成为可能
    
    3. 广泛采用的"先SFT、后RL"范式，在Tool Learning方面，其性能未必优于**直接使用RL**

    4. **格式奖励**不仅仅是一个简单的输出格式检查，它更是一种训练策略。通过强制模型输出结构化的推理过程，引导其从“记忆者”转变为“思考者”，从而获得了处理前所未见的分布外输入的泛化能力（**out of distribution**）；格式检查，就是检查推理过程是否被包含在标签之内，包含\<think>$\cdots$\</think>和\<tool_call>$\cdots$\</tool_call>

    5. R1风格训练方法的scaling law，随模型规模增大而显著提升

    6. no-reason SFT 和 reason-sft 的区别主要在于训练过程中是否强制模型生成结构化推理格式（即是否要求模型在 \<think> 和 \</think> 标签内输出推理步骤）

    7. 对于工具调用类的训练，长的推理链不一定好


## 4. <font color="red">xLAM</font>: A Family of Large Action Models to Empower AI Agent Systems


- **拟解决的问题**：在工具调用数据集和模型方面，缺少开源工作

- **采用的方法**：在**数据处理**方面，采用如下步骤：统一数据格式→数据增强→数据合成→混合训练策略；在**模型训练**方面，采用先SFT（基础能力）、后DPO（输出质量优化）的方法。

- **主要观察**：
    
    1. 采用的数据增强方法：
    
        - 打乱工具列表顺序、参数顺序
        - 使用不同连接标记，如"[START/END OF QUERY]"、"\<query>\</query>"、纯文本
        - 重述任务指令
        - 不同的输出格式（JSON、XML、YAML）


## 5. <font color="blue">（数据合成）</font><font color="red">APIGen</font>: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets


- **拟解决的问题**：现有工作缺少工具调用的训练数据，这篇论文**开源**了**6w条数据**，是xLAM制作数据的方法

- **采用的方法**：构建一个自动化的**数据生成pipeline**，通过多阶段验证确保数据质量。数据生成流程可概括为：

    - **种子数据采样**：从API库中随机采样1个或多个API及其示例QA对，例如query="查询北京天气"，answer="get_weather('北京'，'today')"
    - **标准化格式化**：将种子数据转换为统一JSON格式（含query、answer字段），例如
    
            {
                "query": "查询北京今天的天气", 
                "answer": 
                {
                    "func": "get_weather", 
                    "args": 
                    {
                        "city": "北京", 
                        "date": "today"
                    }
                }
            }

    - **LLM生成新QA对**：根据目标查询风格（如简单/并行调用），选择提示模板引导LLM生成JSON格式的新的函数调用，如
        
            {
                "query": "明天上海温度多少？",
                "answer": 
                    {
                        "func": "get_weather", 
                        "args": 
                        {
                            "city": "上海", 
                            "date": "tomorrow"
                        }
                    }
            }

- **主要观察**：
    
    1. 数据验证过程：
    
        - **格式**：格式检查
        - **执行**：函数执行检查
        - **语义**：用另一个LLM评估执行结果是否真正回答了用户问题

    2. 生成的数据**样本示例**

            {
            "id": 0,
            "query": "Where can I find live giveaways for beta access and games?",
            "answers": [
                {
                "name": "live_giveaways_by_type",
                "arguments": {
                    "type": "beta"
                }
                },
                {
                "name": "live_giveaways_by_type",
                "arguments": {
                    "type": "game"
                }
                }
            ],
            "tools": [
                {
                "name": "live_giveaways_by_type",
                "description": "Retrieve live giveaways from the GamerPower API based on the specified type.",
                "parameters": {
                    "type": {
                    "description": "The type of giveaways to retrieve (e.g., game, loot, beta).",
                    "type": "str",
                    "default": "game"
                    }
                }
                }
            ]
            }


## 6. <font color="blue">（数据合成）</font><font color="red">APIGen-MT</font>: Agentic PIpeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay


- **拟解决的问题**：制作**多轮**工具调用数据集，开源了5k条

- **采用的方法**：一个两阶段框架，通过模拟人机交互生成高质量的多轮代理数据。阶段1生成**任务蓝图**，包含用户质量$q$、真实动作$a_{gt}$、预期输出$o_{gt}$；阶段2基于阶段1的任务蓝图，通过**模拟人工交互**生成真实对话轨迹。

- **主要观察**：
    
    1. 通过**随机游走**来探索不同路径，感觉是生成agent数据的必要步骤
    
    2. 在使用大模型模拟人类时，可能会**随assistant的思路**走，**偏离**了自己**预设的角色和行为**。这种时候，本论文让模拟人类在需要生成回复时，不是只生成一个，而是一次性生成4个回复，并让模拟人类自己（通过一个评判机制）从这4个候选回复中，选出最好的一个（不知道为什么这么做有效果，但不妨先记录下来）

    3. **多轮对话的SFT训练**：收集的每条对话中，在assistant的每一次回复处都切一刀，生成一个训练样本；训练时，用“历史+当前轮问题”作为输入，让模型只学“当前轮回答”部分(只计算assistant本轮回复的loss)


## 7. <font color="red">DiaTool-DPO</font>: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models


- **拟解决的问题**：解决多轮对话中，工具调用所需**参数缺失**问题以及**工具拒绝**问题（用户请求的功能不在工具列表中，模型应拒绝，而非强行调用不匹配工具）

- **采用的方法**：采用多轮直接偏好优化（**DPO**）；定义马尔科夫决策过程，划分5种内部状态（初始、工具选择中、工具选择完成、等待响应、完成）；基于状态转移轨迹将用户查询分为3类（完整工具调用、需槽位填充、拒绝工具调用）；通过对比正确对话流与错误对话流进行学习。

- **主要观察**：
    
    1. 对多轮对话进行加权评分，通过降低后续轮次的权重，突出**早期回答**的**关键性**
    
    2. 对于**槽位填充**任务：坏的轨迹轮次更少，因为模型可能会遗漏需要填写的槽位回答，导致对话提前结束；**好的轨迹轮次更长**，因为模型正确完成了所有必要的问答轮次。对于**相关性判断**任务：好的轨迹在一轮内就完成了，例如模型正确地通过一次工具调用拒绝了不相关的请求；**坏的轨迹轮次更长**，因为模型判断错误，继续进行了不必要或不相关的多轮对话。


    3. 接触**不同难度级别**的数据有助于提升槽位填充能力的训练

    4. 这篇论文没有涉及并行工具调用，参考意义不大


## 8. <font color="red">ToolRM</font>:Outcome Reward Models for Tool-Calling Large Language Models


- **拟解决的问题**：缺少**工具调用场景**下的**奖励模型**工作

- **采用的方法**：生成错误工具调用数据，采用**对比学习**的方法训练专门的结果奖励模型(ToolRM)，并通过Best-of-n采样或数据过滤提升下游工具调用性能

- **主要观察**：
    
    1. **可利用**：这篇论文可以用来对训练数据做**过滤**，比如收集其他已开源的数据，然后用该模型对数据做过滤

    2. 这篇论文训练的奖励模型还挺大的，比如几个B，这么大的奖励模型是否有必要？



## 9. <font color="red">（EGPO）</font>Reasoning through Exploration: A Reinforcement Learning Framework for Robust Function Calling


- **拟解决的问题**：函数调用场景下，**探索复杂推理路径**与保持**策略优化稳定性**之间的平衡

- **采用的方法**：在GRPO的基础上，将**思维链的熵**整合到强化学习的优势计算中，从而激励多样化推理策略的生成；在**格式和答案均正确**的情况下，奖励为1，否则为0；采用**LLM评估**+**AST评估**，来做数据过滤。

- **主要观察**：
    
    1. **思维链熵**，代表模型在**生成推理思维链过程中的不确定性**；值越大，表明模型在生成思维链时探索了更多样的推理路径。

    2. **AST评估**：检查模型是否在不该调用工具时调用了工具，或者在该调用工具时没有调用工具

    3. 这篇论文做了这样一个实验：若模型输出正确的推理格式（即在\<think>和\</think>内封装思维链），则给予0.5额外奖励，但该奖励模式并未产生最佳效果。这表明对于具备指令遵循能力的模型，格式类奖励会造成奖励冗余，反而无法达成最优效果。

    4. 这篇论文的模型很小，但性能很好，有点奇怪哎


## 10. <font color="red">Tool Zero：</font>Training Tool-Augmented LLMs via Pure RL from Scratch


- **拟解决的问题**：使用**SFT增训**的工具增强型LLM，主要**模仿表面调用模式**而非内化推理能力，导致在面对未见过的工具、调用格式或复杂交互时性能显著下降

- **采用的方法**：在GRPO的基础上，使用**渐进式奖励**探索。（1）早期，采用宽松的细粒度奖励，允许模型对语义相关但不完全正确的响应获得部分奖励（2）中期，通过sigmoid衰减函数，混合两种奖励类型（3）后期，采用严格的AST检查

- **主要观察**：
    
    1. **function masking**的操作，将函数名和参数名修改为func_1和param_1，可以减少对浅层名字信息的依赖。这种方法对于提升性能是有用的。

    2. **奖励**的平滑是很重要的

    3. 将**单轮**数据xLAM变为**多轮**的方法：（1）将相关的单轮对话拼接为多轮（2）随机移除某个tool并在后续对话轮次中重新引入（3）随机mask参数值以促使用户澄清（4）随机删除工具、移除参数或修改标准答案中的数值，模拟用户对模型响应的质疑。

    4. 多篇论文中都看到了这个现象：对于tool learning来说，纯粹的RL要优于先SFT后RL的方法

    5. 多数RL的论文，会讲这么一个观点：*监督微调虽然能提升模型在分布内数据上的性能，但会削弱其对未见场景（如新型工具、调用格式等）的泛化能力。泛化能力需通过探索性学习（而非模仿）来获得*

    6. 在强化学习中，指令遵循能力更强的模型并不一定能给工具增强模型带来更大的训练效益。例如，同样是ToolACE数据训练，qwen2.5-7b的性能要优于qwen2.5-7b-instruct。
    
    

## 11. <font color="blue">（数据合成）</font><font color="red">ToolGrad</font>: Efficient Tool-use Dataset Generation with Textual “Gradients”


- **拟解决的问题**：本文主要解决的是**工具使用数据集**生成效率低和效果差的问题。例如，传统方法（如DFS）依赖"查询优先"策略（先生成用户问题，再通过DFS搜索工具链），但DFS探索过程昂贵且无法保证查询可解。

- **采用的方法**：**先构建**有效的**工具链**，再**逆向生成**对应的**用户查询和响应**。每轮迭代扩展工具链$W_t \to W_{t+1}$，模拟机器学习的“前向推理+反向传播”。

    1. **API提议器**
        - **输入**：当前工具链$W_t$随机采样的小批量API
        - **输出**：选出最相关的$m=3$个API及其使用说明
    2. **API执行器**
        - **输入**：$m=3$个API提议
        - **输出**：并行执行API，返回执行报告（包含请求历史+成功/失败标志）
        - **成本**：最昂贵的步骤（需调用工具型LLM）
    3. **API选择器**
        - **输入**：$m=3$个执行报告 + 当前工具链$W_t$
        - **输出**：选择1个最佳API + 决定添加到哪个现有链或新建链
        - **创新**：模拟“文本梯度”，用离散API选择替代数学梯度
    4. **工作流更新器**
        - **更新工具链**：将选中的API（$API_j$）添加到链$C_k$，即$W_{t+1} \leftarrow W_t.add(API_j,C_k)$
        - **生成查询与响应**：基于新工具链$W_{t+1}$，生成用户查询$q_{t+1}$和响应$r_{t+1}$。
    
    例如，假设初始工具链为空：

    - **第1轮**：
        - 提议：天气API、地理编码API、搜索API
        - 执行：天气API成功返回东京天气
        - 选择：将天气API加入新链$C_1$
        - 生成查询：“东京天气”，生成响应：“东京当前气温25度”  
    
    - **第2轮**：
        - 提议：新闻API、翻译API、地图API
        - 执行：地图API成功返回东京景点
        - 选择：将地图API加入链$C_1$
        - 生成查询：“东京天气和热门景点”，生成响应：整合两个API结果 

- **主要观察**：
    
    1. 构建一个**大规模的API池**是有必要的。
    
    2. 将API选择过程类比于梯度下降，通过LLM选择最优API作为“梯度”方向

    3. 这篇论文的写作水平感觉一般，性能还有待验证


## 12. <font color="blue">（数据合成）</font><font color="red">ToolDial</font>: Multi-Turn Dialogue Generation Method for Tool-Augmented Language Models


- **拟解决的问题**：拟合成**多轮对话**、具有**动态交互**的数据集

- **采用的方法**：

    1. 连接API的输入和输出实体来构建API图

    2. 定义16种用户和系统动作类型，并基于这些动作，创建23种可能对话中发生的合理动作序列

    3. 为了生成一个对话，从API图中选择一对API，选择一个动作序列，并未选定API对填充对话状态（参数收集情况）

    4. 用GPI-4o生成自然语言对话


- **主要观察**：
    
    1. 构建API图也是有必要的，但是需要考虑边的数量，**有效的边**的**占比**是很**少**的，需要找到某个函数的输出可作为另一个函数输入的api对.
    
    2. 可利用RapidAPI中提供的函数，并使用StableToolBench做过滤
    
    3. 这篇论文，api的响应是靠大模型模拟的，如果能使用真实的api或者转换为一个包含很多函数的沙箱，会更好。

    4. 这篇论文中，有了api对、动作状态、query后，基本上后续就**全靠GPT-4o模拟**了

    5. 在衡量API和API相连的**边之间的相似度**时，采用如下公式：$$ \text{Edge} = \begin{cases} 1, & \text{if } \text{emb}(d_o, d_i) > t_d \ \land \ \text{emb}(d_o + k_o, d_i + k_i) > t_k \ \land \ \text{LCS}(n_o, n_i) > t_l \\ 0, & \text {otherwise} \end{cases} $$
    其中，$i$和$o$表示输入和输出；$d$，$k$，$n$，$d+k$表示对API的描述、关键词、名称、以及关键词和描述的串联。emb是通过S-BERT模型all-mpnet-base-v2获得的描述文本的嵌入向量。LCS代表最长公共子序列。t表示每个判定标准的阈值。


## 13. <font color="blue">（数据合成）</font><font color="red">ToolMind </font>Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset


- **拟解决的问题**：**合成**数据 + **过滤**现有开源数据，以共同构成**工具使用数据集**

- **采用的方法**：

    1. **合成数据**方面：
    
        1. 收集了**20k**的API，来自xLAM、glaive、ToolACE
        
        2. 建立**API图**（根据函数的输入输出），也会增加一些随机边

        3. 采用**随机游走**，采样函数链

        4. 使用语言模型模拟**用户**、**助手**、**工具**三角色交互

    2. **质量过滤**方面：也是完全通过LLM来过滤
        
        1. 整个**轨迹的过滤**：判断任务是否完成

        2. 每个**轮次**单独评估：移除错误或角色不一致的步骤
        
    3. 这个数据集共有**360k数据**，其中**160k是合成**的，**200k是收集**的；训练方法是**SFT**


- **主要观察**：
    
    1. 在训练时，将多轮对话数据切分为单轮样本进行训练：

        - **切分点**：每个**助手回复(assistant message)处**作为切分点

        - **样本结构**：

            - **完整历史上下文**：从对话开始到当前助手回复的全部内容

            - **当前助手回复**：作为样本的目标输出

            - **丢弃后续内容**：当前助手回复之后的对话轮次被舍弃
        
        - **训练目标**：计算损失时仅针对当前助手回复的token进行，不计算历史上下文部分的损失

        - **处理流程示例**：

            用户：问题1 → 助手：回复1 → 用户：问题2 → 助手：回复2 → 用户：问题3 → 助手：回复3

        - **切分后的训练样本**：

            - 样本1：[用户：问题1] → [助手：回复1]
            - 样本2：[用户: 问题1 + 助手: 回复1 + 用户: 问题2] → [助手: 回复2]
            - 样本3：[完整对话历史] → [助手: 回复3]

    2. 每轮助手的回复包含3部分：**think**、**content**、**tool calls**

    3. 这篇论文收集的数据还挺全面的，可利用其收集的数据再做一下过滤和后处理



## 14. <font color="blue">（数据合成）</font><font color="red">EnvScaler : </font>Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis


- **拟解决的问题**：真实环境访问受限，而LLM模拟环境易出现幻觉和逻辑不一致，人工构建的沙箱很难扩展，因此这篇论文旨在**批量制造交互环境**

- **采用的方法**：合成**轨迹数据**的步骤为：

    1. **SkelBuilder** $\rightarrow$ 生成**环境代码**。

    2. **ScenGenerator** $\rightarrow$ 生成**初始状态**、**任务指令**和**验证函数**。

    3. **轨迹合成**：

        - **教师模型**：论文使用一个较大的模型（如 Qwen3-30B-Thinking）作为“教师”。
        
        - **交互**：让这个教师模型在生成的环境中运行，去解决生成的任务。

        - **筛选**：

            - 教师模型在交互过程中会产生一系列动作序列（即轨迹）

            - 系统运行**验证函数**来检查任务是否成功

            - **只有成功完成任务的轨迹**才会被保留下来

    4. **最终产出**：

        - **SFT数据**：保留下来的约**9K条高质量轨迹**，直接用于训练较小的模型（如 Qwen3-4B/8B）

        - **RL数据**：对于强化学习阶段，不需要预先合成完美轨迹，而是让学生模型自己探索，然后利用生成的验证函数给出的 Reward 进行学习。


- **主要观察**：
    
    1. **SkelBuilder**的任务是自动化地构建一系列**环境集合$\{ E \}$**，每一个合成出来的环境包括3部分：

        - **$E_{exec}$: 可执行程序文件**

            - **含义**：python代码。

            - **作用**：它包含了环境的所有逻辑实现，包括数据库（状态）、每个工具具体怎么运行（函数体）、以及业务规则（比如“没付钱不能发货”的 if-else 判断）。它是真正跑在后台用来处理请求的程序。

        - **$E_{doc}$: 文档**

            - **含义**：环境的**说明书**

            - **作用**：它是给Agent看的。通常是一段自然语言描述，告诉 Agent 这个环境是干嘛的（比如“这是一个图书管理系统”），以及有哪些需要遵守的规则（比如“借书前必须先注册”）
        
        - **$\sum_{tool}$: 工具接口集**

            - **含义**：环境的**操作面板**

            - **作用**：它列出了 Agent 可以调用的所有工具的清单。包括工具的名字（Name）、参数（Parameters，比如需要传 ID 还是 Name）、以及工具的功能描述（Description）。Agent 只能通过这些接口与环境互动，看不见背后的 $E_{exec}$ 代码
    
    2. **ScenGenerator**的任务是将环境骨架（**Skeleton**）转化为可用于训练的具体场景（**Scenario**）：

        - 根据环境的数据结构定义，生成**初始数据状态**

        - 基于初始数据，反向设计出**任务指令**。这包括“一次性给出所有信息”（**Non-Conv**）和“信息缺失需多轮对话”（**Conv**）两种模式。

        - 为每个任务生成一组可执行的python**验证函数**。

    3. 在本篇论文中，多轮对话数据仅用于SFT训练，并没有被用来训练RL。多数的Tool Learning论文也倾向于像EnvScaler这种的**Non-Conv RL**

    4. 这类工作的主要特点是把LLM当做**程序员**，而不是**模拟器**

    5. **RL**在**稍大的模型**上表现更好，比如Qwen3-8B上经过RL的效果要好于Qwen3-1.7B

    6. **non-conv**：任务信息**一次性给全**，无需向用户提问；**conv**：任务信息**不完整**

    7. 这篇论文的**精髓**是它的**验证函数**，但是论文写的有些过于包装，细节还需深挖

    8. 在检查环境生成的是否有效时，代码上采取了如下逻辑：

            初始化环境
                ↓
            for 每个测试步骤 (max_steps次):
                │
                ├─ 记录当前环境状态
                ├─ FuncCallAgent 生成函数调用
                ├─ 在环境中执行函数
                ├─ 记录执行后状态和差异
                ├─ CheckAgent 验证结果
                ├─ 更新统计信息
                └─ 记录步骤日志
                ↓
            转换为结构化测试报告


## 15. <font color="red">DeepSeek-V3.2</font>: Pushing the Frontier of Open Large Language Models

- **在数据合成方面的主要观察**：
    
    1. **Search Agent**数据的构造过程:
    
        1. 挖掘长尾实体（冷门知识）
        
        2. 构造问题（利用搜索工具对选中的冷门实体进行深挖）

        3. 使用不同的LLM生成多个候选答案

        4. 只有满足以下条件的样本会被保留：**标准答案**是**正确**的；所有其他的**候选答案**被验证为**错误**的

    2. **General Agent**数据的构造过程

        1. 对于某个任务类别（比如旅行规划），首先利用搜索工具抓取真实世界的信息（如航班信息、酒店价格），并将其存入一个**沙盒**。

        2. 编写一套专门的Python函数（**Tools**），比如'get_flight_price()'、'check_hotel_availability()'。

        3. **迭代式出题**：生成一个简单任务，并编写配套的**标准答案代码**和**验证代码**。此过程中，标准答案只能调用工具，不能直接查库。逐步增加约束条件（如更紧的预算、更多限制），让任务变难。

        4. 使用 DeepSeek-V3.2 对生成的所有任务进行试做，仅保留 Pass@100 > 0 的任务（即尝试100次至少能做对1次，剔除完全不可能解的任务）

    3. 构造了**1827个环境**和**4417个任务**

    4. Deepseek构造数据时，有这么几个步骤，原话是："The synthesis workflow primarily consists of **environment** and **toolset** construction, **task** synthesis, and **solution** generation"

    4. 在合成数据时，都强调生成**必须依赖**search、code或tool才能解决的问题。例如，必须依靠搜索引擎，才能得到问题的答案；必须通过tool，才能解决问题。这样构造的数据才是有用的。


## 16. <font color="red">Kimi K2</font>: Open Agentic Intelligence

- **在数据合成方面的主要观察**：
    
    1. 收集了**3K**个**MCP tools**，合成了**20K+**个**tools**。

    2. 合成步骤：

        1. 构建**工具库**：从Github中获取了3K个真实MCP工具；从金融交易等关键类别出发，演化出具体的领域，并利用LLM为每个领域合成专用工具。

        2. 生成带有**评分标准**的**任务**：生成从简单到复杂的操作任务，并为每个任务配对明确的评分标准，规定了成功的条件、预期的工具使用模式以及评估检查点。

        3. **多轮轨迹**的生成：通过模拟真实交互，使用LLM生成具有不同沟通风格的用户角色，与代理进行多轮对话。此外，一个工具模拟器负责执行工具调用并提供反馈。


## 17. <font color="red">GLM-4.5</font>: Agentic, Reasoning, and Coding (ARC) Foundation Models

- **在数据合成方面的主要观察**：
    
    1. 关于数据合成方面说的不是很详细，只有这一段：
    
        1. 收集了一系列**智能体框架**、**真实世界的工具API** 和 **MCP 服务器**，并利用LLM**构建和模拟**一批工具（感觉把能用到的方式都用到了，但是不知道数量是多少）
    
        2. 生成任务，包括单步骤和多步骤的工具调用场景 （query应该也是合成的）
    
        3. 利用LLM生成工具调用轨迹。此外，LLM也作为用户模拟器

        4. 对于每个生成的轨迹，使用多个模型来判断任务是否完成。仅保留成功的轨迹



## 18. <font color="blue">（数据合成）</font><font color="red">TOUCAN : </font>Synthesizing 1.5M Tool-Agentic Data From Real-World MCP Environments


- **拟解决的问题**：构建了一个基于**真实MCP（495个）**，150万条轨迹的数据集。

- **采用的方法**：合成**轨迹数据**的步骤为：

    1. **MCP服务器的收集与筛选**：

        - 从**Github**和**Smithery**爬取2800个MCP服务器

        - 只保留支持HTTP流式访问且**不需要第三方鉴权**（不需要注册、不需要API Key）的服务器

        - 进行实际调用测试，**剔除失败**的，最终保留495个。

    2. **任务合成**：

        - 使用**5种**不同的LLM来生成任务，以避免偏见

        - “单MCP”、“跨MCP”、“精选MCP”三种策略（使得**task**不和MCP一一对应）

    3. **任务过滤**：

        - 使用Kimi-K2，从6个维度（工具选择难度、唯一性、问题质量、真实性、可验证性、稳定性）对生成的任务进行1-5分打分，并进行过滤

    4. **轨迹生成**：

        - 使用教师模型（GPT-OSS-120B, Kimi-K2, Qwen3-32B）配合智能体框架，在**MCP服务器**上执行上述任务，并记录下从规划、工具调用、工具返回结果到最终回答的完整交互轨迹。

    5. **轨迹过滤**：

        - **规则过滤**：剔除运行失败、未调用工具、工具报错或包含本地路径隐私的轨迹
        
        - **逻辑校验**：验证工具覆盖率和调用顺序是否符合预期
        
        - **LLM终审**：使用 GPT-OSS-120B 对轨迹的“完整性”和“简洁性”进行评分

    6. **在上述步骤基础上的扩展机制**：

        - **构造无关任务**：将task与错误的工具配对，以训练模型在工具不匹配时学会拒绝回答

        - 改变user的背景，增加**多样性**

        - 拆分任务，将单轮回答扩展为**多轮交互**


- **主要观察**：
    
    1. **帕累托前沿**指的是在不牺牲一个指标的情况下，无法再提升另一个指标的最佳平衡点。这篇论文，指的是模型大小与模型性能之间的权衡

    2. **edge-case tool use**指的是模型需要判断“什么时候不该使用工具”的场景

    3. 构造任务时，设置一个任务需要调用3个工具来解决（这个数量不高）

    4. 这篇论文采用SFT，即使这篇论文制作了150万的数据，也只使用了20w



## 19. <font color="blue">（数据合成）</font><font color="red">AutoForge : </font>Automated Environment Synthesis for Agentic Reinforcement Learning


- **拟解决的问题**：在**数据合成**、**强化学习**算法方面做了创新。

- **采用的方法**：

    1. **合成环境**：

        - 根据工具描述文档生成状态空间

        - 根据工具描述文档+状态空间生成工具集合

        - 状态空间和工具集合共同构成一个环境

    2. **工具序列生成**

        - **采样工具序列**：让LLM判断工具间的可能关系，进而建图，然后随机采样若干条工具序列

        - **两个序列融合**：随机选两条链首尾相连，然后删除重复节点。需要注意的是，当两条链合并时，LLM只会删除那些冗余的节点

            - 如果两个节点是“查同一个ID”，它们是冗余的→合并成一个节点

            - 如果两个节点是“发给A邮件”和“发给B邮件”，它们不是冗余的→保留为两个不同的节点

        - **增加推理节点**：希望tool sequence能够捕捉复杂的推理能力

        - **增加推理边**

    3. **任务合成**：

        1. **环境初始化**，并产生一个**初始的任务**（模糊的意图，根据初始状态和工具调用序列来生成）

        2. **工具序列执行**：根据初始状态，以及每一步的工具调用，计算最终状态

        3. **精炼task**：使用初始状态和最终状态，来精炼task

        背后逻辑：
        
        - 是为了实事求是，有了初始状态和最终状态，再让LLM写最终的任务描述。这时候LLM不是在“编故事”，而是在“做总结”，这样生成的任务描述只包含完成任务所需的**最小必要信息**。**“先射箭（执行），再画靶（写任务）”**，永远比“先画靶，更射箭”更准。通过观察执行结果来反写任务，可以确保训练数据的质量和逻辑自洽性的提高。

        - 如果只根据初始状态和工具序列来生成任务，会缺少参数填充。例如：

            - 场景：假设工具序列是 search_item $\to$ add_to_cart。

            - 问题：如果你直接让 LLM 执行这个序列，LLM 会卡住：“你让我 search，但我搜什么？搜‘苹果’还是‘电脑’？”
            
            - 初始任务的作用：
            
                - $\tilde{Q}_k$（例如“帮我买个红色的杯子”）虽然可能不完美，但它给了 LLM 一个具体的执行线索。

                - 有了这个线索，LLM 在执行 search_item 时，才会自动填入参数 query="red cup"。

                - 没有这个初始任务，执行过程就无法启动。

    4. **环境级相对策略优化算法（ERPO）**:

        - **环境级优势估计**：改进了GRPO算法，不再进行全局比较，而是在每个独立环境内部计算优势，消除了不同环境难度差异带来的评分偏差。

        - **用户错误掩码**：观察user的行为，一旦发现任务失败是由于“模拟用户犯傻”而非Agent无能导致的，就将该条数据屏蔽（mask out），防止agent被错误惩罚。


- **主要观察**：
    
    1. 构造图过程中的连接：
    
        1. 初始图中的工具→工具（边）：
           
            - 逻辑：接口兼容，比如工具A的输出数据类型（比如String）和工具B的输入数据（比如String）类型一致，它们有可能连在一起

        2. **推理节点**：类似于一个虚拟的工具，必须执行一个具体的计算或逻辑判断步骤，从而产生一个原本不存在的新结果。
            
            - 功能：它对**前序信息进行处理，得出更高层级的信息**

            - 例子：如果前面的工具查到了“所有商品的价格”，推理节点负责执行“加法运算”这个动作，算出“总价”。如果没有这个节点，就没有“总价”这个数据。

            - 实现：**通过LLM**

        3. **推理边**：不产生新的知识，而是解决“如何把 A 的输出塞进 B 的输入里”这个问题。

            - 功能：利用**推理过程来决定如何将父节点的输出转化为子节点所需的输入参数**。这里的“推理”指的是“参数匹配的逻辑”，而不是“解决任务的逻辑”。

            - 例子：假设节点 A 输出了“总价”，节点 B 是“付款工具”。推理边负责判断：节点 A 的输出（数字）应该填入节点 B 的 amount（金额）参数中。

            - 实现：**通过LLM**
    
    2. RL相较于SFT的缺点：

        - Sample Efficiency：指的是RL**需要大量试错数据才能学会一点东西**，相较于SFT，需要消耗更多的资源和时间

        - Credit Assignment：指的是任务失败时，模型很难知道到底是哪一步做错了

        - Training Stability: 指的是训练稳定性差

    3. LLM模拟环境的问题在于：**同样的action可能会产生不一致的反馈**，温度设为0只能保证输入完全一致时输出一致，但如果aciton改变了一个标点符号，LLM的输出可能会剧烈变化。

    4. 这篇论文是在**同一个environment下的不同工具**之间进行随机采样。

    5. 这篇论文中，**任务的难度是由tool的个数**来衡量

    6. 这篇论文是纯强化学习的，一个训练样本包括：**（任务、初始状态、目标状态、可用的工具集合）**。有了目标状态，就不需要验证函数了。

    7. 在多轮对话中，使用**LLM模拟user**这个环节也是很重要的，例如采用benchmark中通常使用的GPT4.1，其性能不如使用GPT 5来模拟user。


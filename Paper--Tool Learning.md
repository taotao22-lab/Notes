# 论文笔记--Tool Data Synthesis & Learning

## 1. <font color="red">ToolRL</font>: Reward is All Tool Learning Needs


- **拟解决的问题**：提高大模型的工具调用能力。

- **采用的方法**：**强化学习**进行后训练的方法，**GRPO**（去掉KL约束项，以更快的收敛）；奖励为**格式奖励**和**正确性奖励**的和，$R_{\text{final}} = R_{\text{format}} + R_{\text{correct}} \in [-3, 4]$。

- **主要观察**：
    
    1. **SFT**相比较**RL**的缺点是更容易**过拟合**
    
    2. 更长的推理（“**多思考**”）看起来是好事，但对于“**工具使用**”这类任务，并**不有益**

    3. 对于RL来说，**reward**是很关键的，论文重点探讨reward的不同设计模式

    4. 在对**格式奖励**和**正确性奖励**进行加权时，**不能突变**，突变让GRPO的训练结果变得更差。

## 2. <font color="blue">（数据合成）</font> <font color="red">ToolACE</font>: Winning the Points of LLM Function Calling


- **拟解决的问题**：构造大模型的**函数调用数据集**

- **采用的方法**：1、合成API；2、模拟用户/助手/工具三种角色的真实交互，生成数据；3、过滤，采用规则检查（语法/参数）和模型检查（一致性/幻觉）

- **主要观察**：
    
    1. 对**合成数据的有效使用**可以提高LLM的能力
    
    2. 生成的数据必须与模型的能力相匹配，不能用同一套数据训练所有模型。


## 3. <font color="red">Nemotron-Research-Tool-N1</font>: Exploring Tool-Using Language Models with Reinforced Reasoning


- **拟解决的问题**：提升LLM使用外部工具时的推理能力

- **采用的方法**：强化学习的方法，使用GRPO算法，设计二元奖励函数（**结构和内容完全正确才奖励为1**，否则奖励为0）

- **主要观察**：
    
    1. 把**一个多轮数据**转换为**多个单轮数据**，ToolRL也是这么干的

    2. R1 风格的强化学习**无需精心准备推理轨迹**即可获得推理技能，这使得直接从现有的监督微调数据中训练推理能力成为可能
    
    3. 广泛采用的"先SFT、后RL"范式，在Tool Learning方面，其性能未必优于**直接使用RL**

    4. **格式奖励**不仅仅是一个简单的输出格式检查，它更是一种训练策略。通过强制模型输出结构化的推理过程，引导其从“记忆者”转变为“思考者”，从而获得了处理前所未见的分布外输入的泛化能力（**out of distribution**）；格式检查，就是检查推理过程是否被包含在标签之内，包含\<think>$\cdots$\</think>和\<tool_call>$\cdots$\</tool_call>

    5. R1风格训练方法的scaling law，随模型规模增大而显著提升

    6. no-reason SFT 和 reason-sft 的区别主要在于训练过程中是否强制模型生成结构化推理格式（即是否要求模型在 \<think> 和 \</think> 标签内输出推理步骤）

    7. 对于工具调用类的训练，长的推理链不一定好


## 4. <font color="red">xLAM</font>: A Family of Large Action Models to Empower AI Agent Systems


- **拟解决的问题**：在工具调用数据集和模型方面，缺少开源工作

- **采用的方法**：在**数据处理**方面，采用如下步骤：统一数据格式→数据增强→数据合成→混合训练策略；在**模型训练**方面，采用先SFT（基础能力）、后DPO（输出质量优化）的方法。

- **主要观察**：
    
    1. 采用的数据增强方法：
    
        - 打乱工具列表顺序、参数顺序
        - 使用不同连接标记，如"[START/END OF QUERY]"、"\<query>\</query>"、纯文本
        - 重述任务指令
        - 不同的输出格式（JSON、XML、YAML）


## 5. <font color="blue">（数据合成）</font><font color="red">APIGen</font>: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets


- **拟解决的问题**：现有工作缺少工具调用的训练数据，这篇论文**开源**了**6w条数据**，是xLAM制作数据的方法

- **采用的方法**：构建一个自动化的**数据生成pipeline**，通过多阶段验证确保数据质量。数据生成流程可概括为：

    - **种子数据采样**：从API库中随机采样1个或多个API及其示例QA对，例如query="查询北京天气"，answer="get_weather('北京'，'today')"
    - **标准化格式化**：将种子数据转换为统一JSON格式（含query、answer字段），例如
    
            {
                "query": "查询北京今天的天气", 
                "answer": 
                {
                    "func": "get_weather", 
                    "args": 
                    {
                        "city": "北京", 
                        "date": "today"
                    }
                }
            }

    - **LLM生成新QA对**：根据目标查询风格（如简单/并行调用），选择提示模板引导LLM生成JSON格式的新的函数调用，如
        
            {
                "query": "明天上海温度多少？",
                "answer": 
                    {
                        "func": "get_weather", 
                        "args": 
                        {
                            "city": "上海", 
                            "date": "tomorrow"
                        }
                    }
            }

- **主要观察**：
    
    1. 数据验证过程：
    
        - **格式**：格式检查
        - **执行**：函数执行检查
        - **语义**：用另一个LLM评估执行结果是否真正回答了用户问题

    2. 生成的数据**样本示例**

            {
            "id": 0,
            "query": "Where can I find live giveaways for beta access and games?",
            "answers": [
                {
                "name": "live_giveaways_by_type",
                "arguments": {
                    "type": "beta"
                }
                },
                {
                "name": "live_giveaways_by_type",
                "arguments": {
                    "type": "game"
                }
                }
            ],
            "tools": [
                {
                "name": "live_giveaways_by_type",
                "description": "Retrieve live giveaways from the GamerPower API based on the specified type.",
                "parameters": {
                    "type": {
                    "description": "The type of giveaways to retrieve (e.g., game, loot, beta).",
                    "type": "str",
                    "default": "game"
                    }
                }
                }
            ]
            }


## 6. <font color="blue">（数据合成）</font><font color="red">APIGen-MT</font>: Agentic PIpeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay


- **拟解决的问题**：制作**多轮**工具调用数据集，开源了5k条

- **采用的方法**：一个两阶段框架，通过模拟人机交互生成高质量的多轮代理数据。阶段1生成**任务蓝图**，包含用户质量$q$、真实动作$a_{gt}$、预期输出$o_{gt}$；阶段2基于阶段1的任务蓝图，通过**模拟人工交互**生成真实对话轨迹。

- **主要观察**：
    
    1. 通过**随机游走**来探索不同路径，感觉是生成agent数据的必要步骤
    
    2. 在使用大模型模拟人类时，可能会**随assistant的思路**走，**偏离**了自己**预设的角色和行为**。这种时候，本论文让模拟人类在需要生成回复时，不是只生成一个，而是一次性生成4个回复，并让模拟人类自己（通过一个评判机制）从这4个候选回复中，选出最好的一个（不知道为什么这么做有效果，但不妨先记录下来）

    3. **多轮对话的SFT训练**：收集的每条对话中，在assistant的每一次回复处都切一刀，生成一个训练样本；训练时，用“历史+当前轮问题”作为输入，让模型只学“当前轮回答”部分(只计算assistant本轮回复的loss)


## 7. <font color="red">DiaTool-DPO</font>: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models


- **拟解决的问题**：解决多轮对话中，工具调用所需**参数缺失**问题以及**工具拒绝**问题（用户请求的功能不在工具列表中，模型应拒绝，而非强行调用不匹配工具）

- **采用的方法**：采用多轮直接偏好优化（**DPO**）；定义马尔科夫决策过程，划分5种内部状态（初始、工具选择中、工具选择完成、等待响应、完成）；基于状态转移轨迹将用户查询分为3类（完整工具调用、需槽位填充、拒绝工具调用）；通过对比正确对话流与错误对话流进行学习。

- **主要观察**：
    
    1. 对多轮对话进行加权评分，通过降低后续轮次的权重，突出**早期回答**的**关键性**
    
    2. 对于**槽位填充**任务：坏的轨迹轮次更少，因为模型可能会遗漏需要填写的槽位回答，导致对话提前结束；**好的轨迹轮次更长**，因为模型正确完成了所有必要的问答轮次。对于**相关性判断**任务：好的轨迹在一轮内就完成了，例如模型正确地通过一次工具调用拒绝了不相关的请求；**坏的轨迹轮次更长**，因为模型判断错误，继续进行了不必要或不相关的多轮对话。


    3. 接触**不同难度级别**的数据有助于提升槽位填充能力的训练

    4. 这篇论文没有涉及并行工具调用，参考意义不大


## 8. <font color="red">ToolRM</font>:Outcome Reward Models for Tool-Calling Large Language Models


- **拟解决的问题**：缺少**工具调用场景**下的**奖励模型**工作

- **采用的方法**：生成错误工具调用数据，采用**对比学习**的方法训练专门的结果奖励模型(ToolRM)，并通过Best-of-n采样或数据过滤提升下游工具调用性能

- **主要观察**：
    
    1. **可利用**：这篇论文可以用来对训练数据做**过滤**，比如收集其他已开源的数据，然后用该模型对数据做过滤

    2. 这篇论文训练的奖励模型还挺大的，比如几个B，这么大的奖励模型是否有必要？



## 9. <font color="red">（EGPO）</font>Reasoning through Exploration: A Reinforcement Learning Framework for Robust Function Calling


- **拟解决的问题**：函数调用场景下，**探索复杂推理路径**与保持**策略优化稳定性**之间的平衡

- **采用的方法**：在GRPO的基础上，将**思维链的熵**整合到强化学习的优势计算中，从而激励多样化推理策略的生成；在**格式和答案均正确**的情况下，奖励为1，否则为0；采用**LLM评估**+**AST评估**，来做数据过滤。

- **主要观察**：
    
    1. **思维链熵**，代表模型在**生成推理思维链过程中的不确定性**；值越大，表明模型在生成思维链时探索了更多样的推理路径。

    2. **AST评估**：检查模型是否在不该调用工具时调用了工具，或者在该调用工具时没有调用工具

    3. 这篇论文做了这样一个实验：若模型输出正确的推理格式（即在\<think>和\</think>内封装思维链），则给予0.5额外奖励，但该奖励模式并未产生最佳效果。这表明对于具备指令遵循能力的模型，格式类奖励会造成奖励冗余，反而无法达成最优效果。

    4. 这篇论文的模型很小，但性能很好，有点奇怪哎


## 10. <font color="red">Tool Zero：</font>Training Tool-Augmented LLMs via Pure RL from Scratch


- **拟解决的问题**：使用**SFT增训**的工具增强型LLM，主要**模仿表面调用模式**而非内化推理能力，导致在面对未见过的工具、调用格式或复杂交互时性能显著下降

- **采用的方法**：在GRPO的基础上，使用**渐进式奖励**探索。（1）早期，采用宽松的细粒度奖励，允许模型对语义相关但不完全正确的响应获得部分奖励（2）中期，通过sigmoid衰减函数，混合两种奖励类型（3）后期，采用严格的AST检查

- **主要观察**：
    
    1. **function masking**的操作，将函数名和参数名修改为func_1和param_1，可以减少对浅层名字信息的依赖。这种方法对于提升性能是有用的。

    2. **奖励**的平滑是很重要的

    3. 将**单轮**数据xLAM变为**多轮**的方法：（1）将相关的单轮对话拼接为多轮（2）随机移除某个tool并在后续对话轮次中重新引入（3）随机mask参数值以促使用户澄清（4）随机删除工具、移除参数或修改标准答案中的数值，模拟用户对模型响应的质疑。

    4. 多篇论文中都看到了这个现象：对于tool learning来说，纯粹的RL要优于先SFT后RL的方法

    5. 多数RL的论文，会讲这么一个观点：*监督微调虽然能提升模型在分布内数据上的性能，但会削弱其对未见场景（如新型工具、调用格式等）的泛化能力。泛化能力需通过探索性学习（而非模仿）来获得*

    6. 在强化学习中，指令遵循能力更强的模型并不一定能给工具增强模型带来更大的训练效益。例如，同样是ToolACE数据训练，qwen2.5-7b的性能要优于qwen2.5-7b-instruct。
    
    

## 11. <font color="blue">（数据合成）</font><font color="red">ToolGrad</font>: Efficient Tool-use Dataset Generation with Textual “Gradients”


- **拟解决的问题**：本文主要解决的是**工具使用数据集**生成效率低和效果差的问题。例如，传统方法（如DFS）依赖"查询优先"策略（先生成用户问题，再通过DFS搜索工具链），但DFS探索过程昂贵且无法保证查询可解。

- **采用的方法**：**先构建**有效的**工具链**，再**逆向生成**对应的**用户查询和响应**。每轮迭代扩展工具链$W_t \to W_{t+1}$，模拟机器学习的“前向推理+反向传播”。

    1. **API提议器**
        - **输入**：当前工具链$W_t$随机采样的小批量API
        - **输出**：选出最相关的$m=3$个API及其使用说明
    2. **API执行器**
        - **输入**：$m=3$个API提议
        - **输出**：并行执行API，返回执行报告（包含请求历史+成功/失败标志）
        - **成本**：最昂贵的步骤（需调用工具型LLM）
    3. **API选择器**
        - **输入**：$m=3$个执行报告 + 当前工具链$W_t$
        - **输出**：选择1个最佳API + 决定添加到哪个现有链或新建链
        - **创新**：模拟“文本梯度”，用离散API选择替代数学梯度
    4. **工作流更新器**
        - **更新工具链**：将选中的API（$API_j$）添加到链$C_k$，即$W_{t+1} \leftarrow W_t.add(API_j,C_k)$
        - **生成查询与响应**：基于新工具链$W_{t+1}$，生成用户查询$q_{t+1}$和响应$r_{t+1}$。
    
    例如，假设初始工具链为空：

    - **第1轮**：
        - 提议：天气API、地理编码API、搜索API
        - 执行：天气API成功返回东京天气
        - 选择：将天气API加入新链$C_1$
        - 生成查询：“东京天气”，生成响应：“东京当前气温25度”  
    
    - **第2轮**：
        - 提议：新闻API、翻译API、地图API
        - 执行：地图API成功返回东京景点
        - 选择：将地图API加入链$C_1$
        - 生成查询：“东京天气和热门景点”，生成响应：整合两个API结果 

- **主要观察**：
    
    1. 构建一个**大规模的API池**是有必要的。
    
    2. 将API选择过程类比于梯度下降，通过LLM选择最优API作为“梯度”方向

    3. 这篇论文的写作水平感觉一般，性能还有待验证


## 12. <font color="blue">（数据合成）</font><font color="red">ToolDial</font>: Multi-Turn Dialogue Generation Method for Tool-Augmented Language Models


- **拟解决的问题**：拟合成**多轮对话**、具有**动态交互**的数据集

- **采用的方法**：

    1. 连接API的输入和输出实体来构建API图

    2. 定义16种用户和系统动作类型，并基于这些动作，创建23种可能对话中发生的合理动作序列

    3. 为了生成一个对话，从API图中选择一对API，选择一个动作序列，并未选定API对填充对话状态（参数收集情况）

    4. 用GPI-4o生成自然语言对话


- **主要观察**：
    
    1. 构建API图也是有必要的，但是需要考虑边的数量，**有效的边**的**占比**是很**少**的，需要找到某个函数的输出可作为另一个函数输入的api对.
    
    2. 可利用RapidAPI中提供的函数，并使用StableToolBench做过滤
    
    3. 这篇论文，api的响应是靠大模型模拟的，如果能使用真实的api或者转换为一个包含很多函数的沙箱，会更好。

    4. 这篇论文中，有了api对、动作状态、query后，基本上后续就**全靠GPT-4o模拟**了

    5. 在衡量API和API相连的**边之间的相似度**时，采用如下公式：$$ \text{Edge} = \begin{cases} 1, & \text{if } \text{emb}(d_o, d_i) > t_d \ \land \ \text{emb}(d_o + k_o, d_i + k_i) > t_k \ \land \ \text{LCS}(n_o, n_i) > t_l \\ 0, & \text {otherwise} \end{cases} $$
    其中，$i$和$o$表示输入和输出；$d$，$k$，$n$，$d+k$表示对API的描述、关键词、名称、以及关键词和描述的串联。emb是通过S-BERT模型all-mpnet-base-v2获得的描述文本的嵌入向量。LCS代表最长公共子序列。t表示每个判定标准的阈值。


## 13. <font color="blue">（数据合成）</font><font color="red">ToolMind </font>Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset


- **拟解决的问题**：**合成**数据 + **过滤**现有开源数据，以共同构成**工具使用数据集**

- **采用的方法**：

    1. **合成数据**方面：
    
        1. 收集了**20k**的API，来自xLAM、glaive、ToolACE
        
        2. 建立**API图**（根据函数的输入输出），也会增加一些随机边

        3. 采用**随机游走**，采样函数链

        4. 使用语言模型模拟**用户**、**助手**、**工具**三角色交互

    2. **质量过滤**方面：也是完全通过LLM来过滤
        
        1. 整个**轨迹的过滤**：判断任务是否完成

        2. 每个**轮次**单独评估：移除错误或角色不一致的步骤
        
    3. 这个数据集共有**360k数据**，其中**160k是合成**的，**200k是收集**的；训练方法是**SFT**


- **主要观察**：
    
    1. 在训练时，将多轮对话数据切分为单轮样本进行训练：

        - **切分点**：每个**助手回复(assistant message)处**作为切分点

        - **样本结构**：

            - **完整历史上下文**：从对话开始到当前助手回复的全部内容

            - **当前助手回复**：作为样本的目标输出

            - **丢弃后续内容**：当前助手回复之后的对话轮次被舍弃
        
        - **训练目标**：计算损失时仅针对当前助手回复的token进行，不计算历史上下文部分的损失

        - **处理流程示例**：

            用户：问题1 → 助手：回复1 → 用户：问题2 → 助手：回复2 → 用户：问题3 → 助手：回复3

        - **切分后的训练样本**：

            - 样本1：[用户：问题1] → [助手：回复1]
            - 样本2：[用户: 问题1 + 助手: 回复1 + 用户: 问题2] → [助手: 回复2]
            - 样本3：[完整对话历史] → [助手: 回复3]

    2. 每轮助手的回复包含3部分：**think**、**content**、**tool calls**

    3. 这篇论文收集的数据还挺全面的，可利用其收集的数据再做一下过滤和后处理



## 14. <font color="green">（环境扩展）</font><font color="red">EnvScaler : </font>Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis


- **拟解决的问题**：真实环境访问受限，而LLM模拟环境易出现幻觉和逻辑不一致，人工构建的沙箱很难扩展，因此这篇论文旨在**批量制造交互环境**

- **采用的方法**：合成**轨迹数据**的步骤为：

    1. **SkelBuilder** $\rightarrow$ 生成**环境代码**。

    2. **ScenGenerator** $\rightarrow$ 生成**初始状态**、**任务指令**和**验证函数**。

    3. **轨迹合成**：

        - **教师模型**：论文使用一个强大的模型（如 Qwen3-30B-Thinking）作为“教师”。
        
        - **交互**：让这个教师模型在生成的环境中运行，去解决生成的任务。

        - **筛选**：

            - 教师模型在交互过程中会产生一系列动作序列（即轨迹）

            - 系统运行**验证函数**来检查任务是否成功

            - **只有成功完成任务的轨迹**才会被保留下来

    4. **最终产出**：

        - **SFT数据**：保留下来的约**9K条高质量轨迹**，直接用于训练较小的模型（如 Qwen3-4B/8B）

        - **RL数据**：对于强化学习阶段，不需要预先合成完美轨迹，而是让学生模型自己探索，然后利用生成的验证函数给出的 Reward 进行学习。


- **主要观察**：
    
    1. **SkelBuilder**的任务是自动化地构建一系列**环境集合$\{ E \}$**，每一个合成出来的环境包括3部分：

        - **$E_{exec}$: 可执行程序文件**

            - **含义**：python代码。

            - **作用**：它包含了环境的所有逻辑实现，包括数据库（状态）、每个工具具体怎么运行（函数体）、以及业务规则（比如“没付钱不能发货”的 if-else 判断）。它是真正跑在后台用来处理请求的程序。

        - **$E_{doc}$: 文档**

            - **含义**：环境的**说明书**

            - **作用**：它是给Agent看的。通常是一段自然语言描述，告诉 Agent 这个环境是干嘛的（比如“这是一个图书管理系统”），以及有哪些需要遵守的规则（比如“借书前必须先注册”）
        
        - **$\sum_{tool}$: 工具接口集**

            - **含义**：环境的**操作面板**

            - **作用**：它列出了 Agent 可以调用的所有工具的清单。包括工具的名字（Name）、参数（Parameters，比如需要传 ID 还是 Name）、以及工具的功能描述（Description）。Agent 只能通过这些接口与环境互动，看不见背后的 $E_{exec}$ 代码
    
    2. **ScenGenerator**的任务是将环境骨架（**Skeleton**）转化为可用于训练的具体场景（**Scenario**）：

        - 根据环境的数据结构定义，生成**初始数据状态**

        - 基于初始数据，反向设计出**任务指令**。这包括“一次性给出所有信息”（**Non-Conv**）和“信息缺失需多轮对话”（**Conv**）两种模式。

        - 为每个任务生成一组可执行的python**验证函数**。

    3. 在本篇论文中，多轮对话数据仅用于SFT训练，并没有被用来训练RL。多数的Tool Learning论文也倾向于像EnvScaler这种的**Non-Conv RL**

    4. 这类工作的主要特点是把LLM当做**程序员**，而不是**模拟器**

    5. **RL**在**稍大的模型**上表现更好，比如Qwen3-8B上经过RL的效果要好于Qwen3-1.7B

    6. **non-conv**：任务信息**一次性给全**，无需向用户提问；**conv**：任务信息**不完整**

    7. 这篇论文的**精髓**是它的**验证函数**，但是论文写的有些过于包装，细节还需深挖